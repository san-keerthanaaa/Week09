# -*- coding: utf-8 -*-
"""Week 09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuuUmoU-RKtef6UrjJc8OlaoBRyPSr00

1.
"""

# Commented out IPython magic to ensure Python compatibility.
from IPython.display import display
# %run Machine_Learning_Review_Classification.ipynb

get_results(result_scores)

"""Among all the classification models evaluated, Logistic_L1_C_10 delivered the strongest performance overall. It attained a training accuracy of 0.7347 and a test accuracy of 0.718, the highest among the models assessed. These results indicate that the model effectively balances learning from the training data while maintaining strong generalization to new, unseen data.

2.
"""

patient_df = df_patient.copy()

patient_df["Gender"] = patient_df["Gender"].map({'male': 0, 'female': 1})
patient_df["Race"] = patient_df["Race"].map({'white':0, 'hispanic':1, 'black':2, 'other':3})

X = patient_df[list(vars_left)]
y= patient_df['mortality']
X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(y), random_state=20, test_size=0.20)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

import time
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Dictionary to store results
result_scores = {}

# Function to create and evaluate a logistic regression model with a specific solver
def create_linear_model(solver_name):
    model = LogisticRegression(
        fit_intercept=True,
        penalty=None,
        solver=solver_name,
        max_iter=10000
    )

    start_time = time.time()
    model.fit(X_train, y_train)
    end_time = time.time()

    train_accuracy = accuracy_score(y_train, model.predict(X_train))
    test_accuracy = accuracy_score(y_test, model.predict(X_test))
    elapsed_time = end_time - start_time

    result_scores[solver_name] = (train_accuracy, test_accuracy, elapsed_time)

# Function to display results
def print_results():
    header = f"\n{'Solver':20} {'Training Subset Accuracy':25} {'Test Subset Accuracy':25} {'Time (s)':10}"
    print(header)
    print("-" * len(header))
    for solver, (train_acc, test_acc, duration) in result_scores.items():
        print(f"{solver:20} {train_acc:<25.4f} {test_acc:<25.4f} {duration:<10.4f}")

# Run models with different solvers
solvers = ["lbfgs", "newton-cg", "newton-cholesky", "sag", "saga"]
for solver in solvers:
    create_linear_model(solver)

# Display the results
print_results()

"""3. Compare the results of the models in terms of their accuracy (use this as the performance metric to assess generalizability error on the holdout subset) and the time taken (use appropriate timing function). Summarize your results via a table with the following structure:"""

print_results()

"""4. Among the solvers tested, newton-cholesky delivered the best overall performance. Although all solvers produced very similar accuracy on both the training and test sets, the variation in execution time was notable. newton-cholesky stood out by offering one of the top test accuracies (0.7355) while also being the fastest, completing the task in only 0.0336 seconds. While sag and saga achieved a slightly higher test accuracy (0.7358), they required significantly more time to execute. The final ranking prioritized both generalization performance and computational efficiency, with greater weight placed on the model's ability to generalize well and run quickly."""